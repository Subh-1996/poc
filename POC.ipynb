{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d84c23b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>ppu</th>\n",
       "      <th>batters</th>\n",
       "      <th>topping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>donut</td>\n",
       "      <td>Cake</td>\n",
       "      <td>0.55</td>\n",
       "      <td>{'batter': [{'id': '1001', 'type': 'Regular'},...</td>\n",
       "      <td>[{'id': '5001', 'type': 'None'}, {'id': '5002'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>donut</td>\n",
       "      <td>Raised</td>\n",
       "      <td>0.55</td>\n",
       "      <td>{'batter': [{'id': '1001', 'type': 'Regular'}]}</td>\n",
       "      <td>[{'id': '5001', 'type': 'None'}, {'id': '5002'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>donut</td>\n",
       "      <td>Old Fashioned</td>\n",
       "      <td>0.55</td>\n",
       "      <td>{'batter': [{'id': '1001', 'type': 'Regular'},...</td>\n",
       "      <td>[{'id': '5001', 'type': 'None'}, {'id': '5002'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   type           name   ppu  \\\n",
       "0   1  donut           Cake  0.55   \n",
       "1   2  donut         Raised  0.55   \n",
       "2   3  donut  Old Fashioned  0.55   \n",
       "\n",
       "                                             batters  \\\n",
       "0  {'batter': [{'id': '1001', 'type': 'Regular'},...   \n",
       "1    {'batter': [{'id': '1001', 'type': 'Regular'}]}   \n",
       "2  {'batter': [{'id': '1001', 'type': 'Regular'},...   \n",
       "\n",
       "                                             topping  \n",
       "0  [{'id': '5001', 'type': 'None'}, {'id': '5002'...  \n",
       "1  [{'id': '5001', 'type': 'None'}, {'id': '5002'...  \n",
       "2  [{'id': '5001', 'type': 'None'}, {'id': '5002'...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id           int64\n",
      "type        object\n",
      "name        object\n",
      "ppu        float64\n",
      "batters     object\n",
      "topping     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'C:/Users/subha/OneDrive/Desktop/sample_json.json'\n",
    "\n",
    "# Read the JSON file into a Pandas DataFrame\n",
    "df = pd.read_json(json_file_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)\n",
    "\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34d06076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>ppu</th>\n",
       "      <th>batters</th>\n",
       "      <th>topping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>donut</td>\n",
       "      <td>Cake</td>\n",
       "      <td>0.55</td>\n",
       "      <td>{'batter': [{'id': '1001', 'type': 'Regular'},...</td>\n",
       "      <td>[{'id': '5001', 'type': 'None'}, {'id': '5002'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>donut</td>\n",
       "      <td>Raised</td>\n",
       "      <td>0.55</td>\n",
       "      <td>{'batter': [{'id': '1001', 'type': 'Regular'}]}</td>\n",
       "      <td>[{'id': '5001', 'type': 'None'}, {'id': '5002'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>donut</td>\n",
       "      <td>Old Fashioned</td>\n",
       "      <td>0.55</td>\n",
       "      <td>{'batter': [{'id': '1001', 'type': 'Regular'},...</td>\n",
       "      <td>[{'id': '5001', 'type': 'None'}, {'id': '5002'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   type           name   ppu  \\\n",
       "0   1  donut           Cake  0.55   \n",
       "1   2  donut         Raised  0.55   \n",
       "2   3  donut  Old Fashioned  0.55   \n",
       "\n",
       "                                             batters  \\\n",
       "0  {'batter': [{'id': '1001', 'type': 'Regular'},...   \n",
       "1    {'batter': [{'id': '1001', 'type': 'Regular'}]}   \n",
       "2  {'batter': [{'id': '1001', 'type': 'Regular'},...   \n",
       "\n",
       "                                             topping  \n",
       "0  [{'id': '5001', 'type': 'None'}, {'id': '5002'...  \n",
       "1  [{'id': '5001', 'type': 'None'}, {'id': '5002'...  \n",
       "2  [{'id': '5001', 'type': 'None'}, {'id': '5002'...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas.core.common import flatten\n",
    "\n",
    "# Step 1: Read the JSON file into a DataFrame\n",
    "json_file_path = 'C:/Users/subha/OneDrive/Desktop/sample_json.json'\n",
    "df = pd.read_json(json_file_path)\n",
    "\n",
    "display(df)\n",
    "\n",
    "# Step 2: Function to check if a column contains nested structures\n",
    "def has_nested_structures(cell):\n",
    "    return any(isinstance(item, (list, dict)) for item in flatten([cell]))\n",
    "\n",
    "# Step 3: Identify columns that are not fully expanded\n",
    "columns_to_expand = [col for col in df.columns if any(df[col].apply(has_nested_structures))]\n",
    "\n",
    "print(columns_to_expand)\n",
    "\n",
    "# # Step 4: Expand the columns that are not fully expanded\n",
    "# for col in columns_to_expand:\n",
    "#     expanded_col = pd.json_normalize(df[col], sep='_', max_level=1)\n",
    "#     df = pd.concat([df, expanded_col], axis=1).drop(columns=[col])\n",
    "\n",
    "# # Now, 'df' contains the fully expanded DataFrame\n",
    "# # You can further process or load it as needed\n",
    "# print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a997a755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types of Columns:\n",
      "id           int64\n",
      "type        object\n",
      "name        object\n",
      "ppu        float64\n",
      "batters     object\n",
      "topping     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assume df is your DataFrame obtained by reading the JSON file\n",
    "df = pd.read_json('C:/Users/subha/OneDrive/Desktop/sample_json.json')\n",
    "\n",
    "# Display the data types of each column\n",
    "column_data_types = df.dtypes\n",
    "\n",
    "print(\"Data Types of Columns:\")\n",
    "print(column_data_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "717bf5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types of Columns:\n",
      "id           int64\n",
      "type        object\n",
      "name        object\n",
      "ppu        float64\n",
      "batters     object\n",
      "topping     object\n",
      "dtype: object\n",
      "DataFrame Information:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>ppu</th>\n",
       "      <th>batters</th>\n",
       "      <th>topping</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>donut</td>\n",
       "      <td>Cake</td>\n",
       "      <td>0.55</td>\n",
       "      <td>{'batter': [{'id': '1001', 'type': 'Regular'},...</td>\n",
       "      <td>[{'id': '5001', 'type': 'None'}, {'id': '5002'...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>donut</td>\n",
       "      <td>Raised</td>\n",
       "      <td>0.55</td>\n",
       "      <td>{'batter': [{'id': '1001', 'type': 'Regular'}]}</td>\n",
       "      <td>[{'id': '5001', 'type': 'None'}, {'id': '5002'...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>donut</td>\n",
       "      <td>Old Fashioned</td>\n",
       "      <td>0.55</td>\n",
       "      <td>{'batter': [{'id': '1001', 'type': 'Regular'},...</td>\n",
       "      <td>[{'id': '5001', 'type': 'None'}, {'id': '5002'...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   type           name   ppu  \\\n",
       "0   1  donut           Cake  0.55   \n",
       "1   2  donut         Raised  0.55   \n",
       "2   3  donut  Old Fashioned  0.55   \n",
       "\n",
       "                                             batters  \\\n",
       "0  {'batter': [{'id': '1001', 'type': 'Regular'},...   \n",
       "1    {'batter': [{'id': '1001', 'type': 'Regular'}]}   \n",
       "2  {'batter': [{'id': '1001', 'type': 'Regular'},...   \n",
       "\n",
       "                                             topping  is_list  is_dict  \n",
       "0  [{'id': '5001', 'type': 'None'}, {'id': '5002'...    False     True  \n",
       "1  [{'id': '5001', 'type': 'None'}, {'id': '5002'...    False     True  \n",
       "2  [{'id': '5001', 'type': 'None'}, {'id': '5002'...    False     True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assume df is your DataFrame obtained by reading the JSON file\n",
    "df = pd.read_json('C:/Users/subha/OneDrive/Desktop/sample_json.json')\n",
    "\n",
    "# Display the data types of each column\n",
    "column_data_types = df.dtypes\n",
    "\n",
    "print(\"Data Types of Columns:\")\n",
    "print(column_data_types)\n",
    "# Function to check if an element is a list\n",
    "\n",
    "def is_list(element):\n",
    "    return isinstance(element, list)\n",
    "\n",
    "# Function to check if an element is a dictionary\n",
    "def is_dict(element):\n",
    "    return isinstance(element, dict)\n",
    "\n",
    "# Check if 'your_column' contains lists\n",
    "is_list_column = df['batters'].apply(lambda x: any(is_list(item) for item in x))\n",
    "\n",
    "# Check if 'your_column' contains dictionaries\n",
    "is_dict_column = df['topping'].apply(lambda x: any(is_dict(item) for item in x))\n",
    "\n",
    "# Display the DataFrame along with additional information\n",
    "df_info = pd.concat([df, is_list_column.rename('is_list'), is_dict_column.rename('is_dict')], axis=1)\n",
    "\n",
    "print(\"DataFrame Information:\")\n",
    "display(df_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "816251d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------+----+--------------------+-----+\n",
      "|             batters|  id|         name| ppu|             topping| type|\n",
      "+--------------------+----+-------------+----+--------------------+-----+\n",
      "|{[{1001, Regular}...|0001|         Cake|0.55|[{5001, None}, {5...|donut|\n",
      "| {[{1001, Regular}]}|0002|       Raised|0.55|[{5001, None}, {5...|donut|\n",
      "|{[{1001, Regular}...|0003|Old Fashioned|0.55|[{5001, None}, {5...|donut|\n",
      "+--------------------+----+-------------+----+--------------------+-----+\n",
      "\n",
      "root\n",
      " |-- batters: struct (nullable = true)\n",
      " |    |-- batter: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- type: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- ppu: double (nullable = true)\n",
      " |-- topping: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def read_json_to_spark(json_file_path):\n",
    "    # Create a Spark session\n",
    "    spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "    # Read the JSON file into a PySpark DataFrame\n",
    "    spark_df = spark.read.option(\"multiline\",\"true\").format(\"json\").load(json_file_path)\n",
    "\n",
    "    return spark_df\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'C:/Users/subha/OneDrive/Desktop/sample_json.json'\n",
    "\n",
    "# Call the function to read and parse the JSON file\n",
    "spark_df = read_json_to_spark(json_file_path)\n",
    "\n",
    "# Display the PySpark DataFrame\n",
    "spark_df.show()\n",
    "\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "528e039a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing :batters Type :<class 'pyspark.sql.types.StructType'>\n",
      "Processing :topping Type :<class 'pyspark.sql.types.ArrayType'>\n",
      "Processing :topping Type :<class 'pyspark.sql.types.StructType'>\n",
      "Processing :batters_batter Type :<class 'pyspark.sql.types.ArrayType'>\n",
      "Processing :batters_batter Type :<class 'pyspark.sql.types.StructType'>\n",
      "+----+----+----+-----+----------+------------------------+-----------------+-------------------+\n",
      "|id  |name|ppu |type |topping_id|topping_type            |batters_batter_id|batters_batter_type|\n",
      "+----+----+----+-----+----------+------------------------+-----------------+-------------------+\n",
      "|0001|Cake|0.55|donut|5001      |None                    |1001             |Regular            |\n",
      "|0001|Cake|0.55|donut|5001      |None                    |1002             |Chocolate          |\n",
      "|0001|Cake|0.55|donut|5001      |None                    |1003             |Blueberry          |\n",
      "|0001|Cake|0.55|donut|5001      |None                    |1004             |Devil's Food       |\n",
      "|0001|Cake|0.55|donut|5002      |Glazed                  |1001             |Regular            |\n",
      "|0001|Cake|0.55|donut|5002      |Glazed                  |1002             |Chocolate          |\n",
      "|0001|Cake|0.55|donut|5002      |Glazed                  |1003             |Blueberry          |\n",
      "|0001|Cake|0.55|donut|5002      |Glazed                  |1004             |Devil's Food       |\n",
      "|0001|Cake|0.55|donut|5005      |Sugar                   |1001             |Regular            |\n",
      "|0001|Cake|0.55|donut|5005      |Sugar                   |1002             |Chocolate          |\n",
      "|0001|Cake|0.55|donut|5005      |Sugar                   |1003             |Blueberry          |\n",
      "|0001|Cake|0.55|donut|5005      |Sugar                   |1004             |Devil's Food       |\n",
      "|0001|Cake|0.55|donut|5007      |Powdered Sugar          |1001             |Regular            |\n",
      "|0001|Cake|0.55|donut|5007      |Powdered Sugar          |1002             |Chocolate          |\n",
      "|0001|Cake|0.55|donut|5007      |Powdered Sugar          |1003             |Blueberry          |\n",
      "|0001|Cake|0.55|donut|5007      |Powdered Sugar          |1004             |Devil's Food       |\n",
      "|0001|Cake|0.55|donut|5006      |Chocolate with Sprinkles|1001             |Regular            |\n",
      "|0001|Cake|0.55|donut|5006      |Chocolate with Sprinkles|1002             |Chocolate          |\n",
      "|0001|Cake|0.55|donut|5006      |Chocolate with Sprinkles|1003             |Blueberry          |\n",
      "|0001|Cake|0.55|donut|5006      |Chocolate with Sprinkles|1004             |Devil's Food       |\n",
      "+----+----+----+-----+----------+------------------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import col, explode_outer\n",
    "\n",
    "#function to flatten/explode the non-linear data type columns\n",
    "def flatten(df):\n",
    "  complex_fields = dict([(field.name,field.dataType)\n",
    "                        for field in df.schema.fields\n",
    "                        if type(field.dataType) == StructType or type(field.dataType) == ArrayType])\n",
    "  \n",
    "  while(len(complex_fields)!=0):\n",
    "    col_name = list(complex_fields.keys())[0]\n",
    "    print(\"Processing :\"+col_name+\" Type :\"+str(type(complex_fields[col_name])))\n",
    "    \n",
    "    #if StructType then flatten struct and if ArrayType then explode the arrays into sub elements\n",
    "    if(type(complex_fields[col_name]) == StructType):\n",
    "      expanded = [col(col_name+'.'+k).alias(col_name+'_'+k) for k in [n.name for n in complex_fields[col_name]]]\n",
    "      df = df.select(\"*\",*expanded).drop(col_name)\n",
    "      \n",
    "    #if ArrayType\n",
    "    elif(type(complex_fields[col_name]) == ArrayType ):\n",
    "      df = df.withColumn(col_name, explode_outer(col_name))\n",
    "      \n",
    "    #recompute remaining Complex fields in Schema\n",
    "    complex_fields = dict([(field.name,field.dataType)\n",
    "                          for field in df.schema.fields\n",
    "                          if type(field.dataType) == StructType or type(field.dataType) == ArrayType])\n",
    "    \n",
    "  return df\n",
    "\n",
    " \n",
    "df = flatten(spark_df)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db1a6c08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in c:\\users\\subha\\anaconda3\\lib\\site-packages (5.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f690f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C:\\Users\\subha\\OneDrive\\Desktop\\sample_yaml.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "88d6b1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Load_Index': 1, 'Table': 'dbo.sample_json', 'Load Type': 'Flat File', 'File Format': 'json', 'Source File Location': 'C:\\\\Users\\\\subha\\\\OneDrive\\\\Desktop\\\\sample_yaml.yml', 'multiline': True, 'Indentation': None}\n",
      "Flat File\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Specify the path to your YAML file\n",
    "yaml_file_path = 'C:/Users/subha/OneDrive/Desktop/sample_yaml.yml'\n",
    "\n",
    "# Read the YAML file\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_data = yaml.safe_load(file)\n",
    "\n",
    "# Display the content of the YAML file\n",
    "print(yaml_data)\n",
    "print(yaml_data['Load Type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "34f8e1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Load_Index': 1, 'Table': 'dbo.sample_json', 'Load Type': 'Flat File', 'File Format': 'json', 'File Directoy': 'C:\\\\Users\\\\subha\\\\OneDrive\\\\Desktop\\\\', 'File Name': 'sample_yaml.yml', 'multiline': True, 'Indentation': None}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Define the path to your YAML file\n",
    "yaml_file_path = 'C:/Users/subha/OneDrive/Desktop/sample_yaml.yml'\n",
    "\n",
    "def yaml_file_read(yaml_file_path) :\n",
    "    # Read the YAML file\n",
    "    with open(yaml_file_path, 'r') as file:\n",
    "        yaml_data = yaml.safe_load(file)\n",
    "        \n",
    "    return yaml_data\n",
    "\n",
    "print(yaml_file_read(yaml_file_path))\n",
    "\n",
    "# print(yaml_data['File Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "042cebd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\nSince Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().\n      ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21736/1477507903.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Display the PySpark DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mspark_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mspark_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \nSince Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().\n      "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def read_json_to_spark(json_file_path):\n",
    "    # Create a Spark session\n",
    "    spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "    # Read the JSON file into a PySpark DataFrame\n",
    "    spark_df = spark.read.option(\"multiline\",\"true\").format(\"json\").load(json_file_path)\n",
    "\n",
    "    return spark_df\n",
    "\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'C:/Users/subha/OneDrive/Desktop/config_json.json'\n",
    "\n",
    "# Call the function to read and parse the JSON file\n",
    "spark_df = read_json_to_spark(json_file_path)\n",
    "\n",
    "# Display the PySpark DataFrame\n",
    "spark_df.show()\n",
    "\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "31337190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+---------+--------------------+---------+\n",
      "|File Format|       File Name|Load Type|     _corrupt_record|multiline|\n",
      "+-----------+----------------+---------+--------------------+---------+\n",
      "|       null|            null|     null|                   [|     null|\n",
      "|       null|            null|     null|                   {|     null|\n",
      "|       null|            null|     null|    \"Load_Index\": 1,|     null|\n",
      "|       null|            null|     null|    \"Table\": \"dbo...|     null|\n",
      "|       null|            null|     null|           \"Info\": [|     null|\n",
      "|       null|            null|Flat File|                null|     null|\n",
      "|       json|            null|     null|                null|     null|\n",
      "|       null|            null|     null|      {\"File Dire...|     null|\n",
      "|       null|config_json.json|     null|                null|     null|\n",
      "|       null|            null|     null|                null|      yes|\n",
      "|       null|            null|     null|               \\t  ]|     null|\n",
      "|       null|            null|     null|                   }|     null|\n",
      "|       null|            null|     null|                   ]|     null|\n",
      "+-----------+----------------+---------+--------------------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[File Format: string, File Name: string, Load Type: string, _corrupt_record: string, multiline: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "json_file_path = 'C:/Users/subha/OneDrive/Desktop/config_json.json'\n",
    "\n",
    "# Read the JSON file into a PySpark DataFrame\n",
    "spark_df = spark.read.format(\"json\").load(json_file_path)\n",
    "\n",
    "# # Call the function to read and parse the JSON file\n",
    "# spark_df = read_json_to_spark(json_file_path)\n",
    "\n",
    "# Display the PySpark DataFrame\n",
    "spark_df.show()\n",
    "\n",
    "display(spark_df)\n",
    "\n",
    "# spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c8434584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-----------+---------+-----------------------+-----------------+--------------------+\n",
      "|   Source|         Target|Model Query|Load Type|Primary Key Combination|Connection String|       File Location|\n",
      "+---------+---------------+-----------+---------+-----------------------+-----------------+--------------------+\n",
      "|Flat File|dbo.sample_json|       null|     Full|                   null|             null|C:\\\\Users\\\\subha\\...|\n",
      "+---------+---------------+-----------+---------+-----------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mdd_config_file_path = \"C:/Users/subha/OneDrive/Desktop/MDD_Config.csv\"\n",
    "\n",
    "df = spark.read.csv(mdd_config_file_path, header=True)\n",
    "\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
